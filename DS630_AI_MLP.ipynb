{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS630_AI_MLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOvobRCWoK7OmRSg0KZ54kz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amal211/DS_level2/blob/main/DS630_AI_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEcv_lF5Pcg6"
      },
      "source": [
        "# import the required libraries\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "import pandas as pd\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "# upload the dataset\r\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/Amal211/DS_level2/main/california_housing_sale_regression.csv')\r\n",
        "\r\n",
        "missing_values = df.apply(lambda x: sum(x.isnull()), axis=0)   # find missing values\r\n",
        "\r\n",
        "print('\\nThe miising values by columns:\\n\\n{}\\n\\n' .format(missing_values))\r\n",
        "\r\n",
        "# replace missing values with the mean value\r\n",
        "df['total_bedrooms'].fillna(df['total_bedrooms'].mean(), inplace=True)\r\n",
        "\r\n",
        "missing_values = df.apply(lambda x: sum(x.isnull()), axis=0)   # find missing values\r\n",
        "\r\n",
        "print('\\nThe miising values by columns:\\n\\n{}\\n\\n' .format(missing_values))\r\n",
        "\r\n",
        "# drop the 'ocean_proximity' column\r\n",
        "df.drop('ocean_proximity', axis=1, inplace=True)\r\n",
        "\r\n",
        "def main():\r\n",
        "    \r\n",
        "    # identify features X and the target Y\r\n",
        "    X = df.iloc[:, 0:7]\r\n",
        "    Y = df['median_house_value']\r\n",
        "\r\n",
        "    # create scaler to scaling the dataset variables to improve the model performance [2]\r\n",
        "    scaler = StandardScaler()\r\n",
        "\r\n",
        "    # fit and transform the dataset\r\n",
        "    standardized = scaler.fit_transform(df)\r\n",
        "\r\n",
        "    # inverse transform\r\n",
        "    inverse = scaler.inverse_transform(standardized)\r\n",
        "\r\n",
        "    # We created a sequintial model that has a stack of linear layers [3].\r\n",
        "    # The model has 3 dense layers of fully connected neurons \r\n",
        "    # first layer with 16 outputs\r\n",
        "    # second layer with 8 outputs\r\n",
        "    # last layer with 1 output\r\n",
        "\r\n",
        "    # we use the ReLU activation function in the first and second layer\r\n",
        "    # ReLu or Rectified Linear Unit has a low computational cost\r\n",
        "    # since it activate neurons with possitive values only.\r\n",
        "    # negative values will result zero and correponding neurons will not get activated [4].\r\n",
        "\r\n",
        "    # we used a linear function in the last layer to get \r\n",
        "    # a prediction of numerical value (negative or possitive numbers).\r\n",
        "    # we did not use it in input and hidden layers \r\n",
        "    # since it does not allow for using backpropagation to train the data [5].\r\n",
        "\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(16, input_shape=(7,), activation='relu'))\r\n",
        "    model.add(Dense(8, activation='relu'))\r\n",
        "    model.add(Dense(1, activation='linear'))\r\n",
        "\r\n",
        "\r\n",
        "    # build the model and start training.\r\n",
        "    # we use adam or Adaptive Moment Estimation that use adaptive learning rate, \r\n",
        "    # to update network weights iterative based in training data [6].\r\n",
        "\r\n",
        "    # loss is the value that we seek to minimize during the model training,\r\n",
        "    # the lower value of loss the closer is the prediction.\r\n",
        "    # we use mean_absolute_error and mean_sequared_error to compute the loss. \r\n",
        "    # mean_absolute_error is equal to the mean of absolute difference between y_label and y_prediction,\r\n",
        "    # while the mean_squared_error is the mean of squared difference between y_label and y_prediction [7].\r\n",
        "\r\n",
        "    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_squared_error'])\r\n",
        "    model.fit(X, Y, epochs=250, batch_size=10, verbose=2, validation_split=0.2)\r\n",
        "\r\n",
        "    # compute the frequency of y_prediction matches y_lable\r\n",
        "    accuracy = model.evaluate(X, Y, verbose=0)\r\n",
        "\r\n",
        "    print(accuracy)\r\n",
        "\r\n",
        "    # make a prediction\r\n",
        "    y_prediction = model.predict(X)\r\n",
        "    try:\r\n",
        "        print('\\nThe predicted median house values:\\n\\n{}\\n\\n' .format(y_prediction))\r\n",
        "    except:\r\n",
        "        print('Exception!')\r\n",
        "    \r\n",
        "    # show the metric\r\n",
        "    plt.plot(model.history.history['mean_squared_error'])\r\n",
        "    plt.xlabel('epoches')\r\n",
        "    plt.ylabel('mean_squared_error')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    main()\r\n",
        "\r\n",
        "# The output\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}